# Data Professional
I’m a data professional with expertise in data engineering, cloud technologies, and machine learning. I hold a Master’s in Data Analytics and certifications in Oracle OCI Generative AI and Advanced SQL. My work spans industries like finance, education, and healthcare, focusing on building ETL pipelines, automating workflows, and delivering interactive dashboards.

## Education
- Master's of Science in Data Analytics (Clark University, Worcester, USA) (2022-2024) 
- Bachelor's in Computer Applications (SVSU, Meerut, India) 

## Work Experience

**Data Analyst @ ITMC Solutions Inc. | New Jersey, USA** 
(Sep 2024-Current)

As a Data Analyst with over 2 years of experience, I have played a key role in transforming raw data into valuable business insights. My work focuses on data analysis, reporting, and process optimization, primarily using SQL, Python, and Tableau.
- Data Analysis & Reporting: Leveraged SQL (MySQL, Oracle DB) to write complex queries for extracting, transforming, and loading data from various databases. Developed interactive Tableau dashboards to provide real-time insights and visualizations that enhanced decision-making for stakeholders across departments.
- Data Quality & Validation: Ensured data integrity by performing thorough validation checks and cleaning datasets. I worked extensively with large datasets, applying techniques like data normalization and deduplication to ensure high-quality outputs.
- Cross-functional Collaboration: Worked closely with business units to gather and document requirements, translating them into actionable insights. This resulted in process improvements, data-driven strategies, and optimized performance tracking.
- Advanced Visualizations: Produced detailed reports using Tableau and Power BI, focusing on performance metrics and trends. These reports were instrumental in driving business growth and operational efficiency.

**Data Engineer @ Ample Softech Systems | Pune, India**
(Jan 2020-Mar 2022)

As a Data Engineer, I focused on building, maintaining, and optimizing data pipelines and infrastructure to enable seamless data flow across the organization. My work involved handling large-scale datasets, improving data architecture, and supporting data-driven decision-making.
- ETL Pipeline Development: Designed and implemented scalable ETL pipelines using Python, MySQL, and Flask to extract, transform, and load data from multiple sources into centralized databases. This improved data accessibility for the analytics team and reduced data processing time by 22%.
- Data Integration & Automation: Automated key data workflows with tools like Docker and Kubernetes, ensuring efficient and reliable data movement across various platforms. I utilized AWS services like S3 for storage and Lambda for event-driven processing, enhancing the scalability and speed of data operations.
- Database Management: Optimized the performance of MySQL databases by restructuring tables, indexing, and tuning queries to ensure efficient storage and retrieval of data. This reduced query response time significantly and improved system performance.
- Data Governance & Validation: Ensured data quality and compliance through rigorous validation processes and monitoring tools. Developed custom scripts in Python to clean and transform data, ensuring accuracy and consistency across all datasets.
- Collaboration & Reporting: Collaborated with cross-functional teams, including data analysts and product managers, to understand data requirements and deliver tailored solutions. Created automated reports and dashboards that provided actionable insights to stakeholders.

## Data Projects

***Mental Health & Music Analysis Project***

 - This project aimed to examine the relationship between music and mental health across different demographics, with the goal of identifying how music influences emotional well-being. By analyzing data, I sought to uncover trends and correlations that could inform research or therapeutic approaches to mental health.
 - Tech Used: Python (Pandas, NumPy) for data analysis, AWS Glue for efficient data cleaning and transformation, Python for visualizations that made the insights easy to interpret.
 - Techniques Used: I utilized AWS Glue for data preprocessing and handling, ensuring the dataset was clean and well-structured for analysis. Correlation analysis was employed to identify potential links between specific genres of music and mental health outcomes. I also applied demographic segmentation to understand how music preferences impacted different groups 
   based on age, gender, and other factors. Python was instrumental in data manipulation and visualizations that highlighted key findings and trends.
 - Desired Outcome: The goal was to reveal *How different types of music influence mental health across various demographic groups*, providing insights that could be used in future mental health research or in developing therapeutic tools. The project aimed to showcase the potential for personalized music therapy and contribute to a deeper understanding of music’s 
   role in mental wellness.

***Movie Recommendation Engine Application***
 - I built and deployed a movie recommendation engine on localhost using **Flask**, allowing users to get **personalized movie suggestions based on their preferences**. The system leverages the *TMDB dataset of 9000 movies* and offers multiple recommendation methods to enhance the user experience.
 - Key Features:
   Users can select a movies and receive 10 personalized recommendations.
   Movie recommendations based on title search, genre, or language filters.
   Display of ratings for all recommended movies.
   Halloween-themed interface for an engaging user experience.
 - Tech Stack:
   *Flask for backend routing.
    Pandas & NumPy for data preprocessing.
    HTML, CSS for the front-end, 
    TMDB Dataset for movie metadata.*
 - Techniques:
   *Content-Based Filtering for personalized recommendations, 
    Data Preprocessing to ensure clean and accurate data.*
 - Outcome:
   The engine provides flexible, user-friendly movie recommendations with multiple input methods, offering a smooth and enjoyable movie discovery experience.

***Software Fault Detection Using Machine Learning*** [Link](https://github.com/kushagra-619/Software_Fault_Detection_using_Machine_Learning)
 - This capstone project focused on Software Fault Detection through the application of machine learning techniques, specifically comparing the performance of ensemble models against baseline models. The goal was to improve the accuracy of fault detection in software applications, enhancing overall software reliability.
 - Project Goals:
   Fault Detection: Utilize machine learning algorithms to identify and predict software faults based on historical data.
   Model Comparison: Evaluate the performance of various ensemble models (such as Random Forest, AdaBoost, and Gradient Boosting) against traditional baseline models (like logistic regression and decision trees).
 - Tech Stack:
   Programming Languages: Python for developing and implementing machine learning algorithms.
   Machine Learning Libraries: Scikit-learn for model building and evaluation, along with Pandas and NumPy for data manipulation.
   Visualization Tools: Matplotlib and Seaborn for visualizing model performance and results.
 - Techniques Used:
   Data Preprocessing: Cleaned and prepared the dataset to ensure high-quality input for model training.
   Feature Engineering: Selected and created features that improved model accuracy.
   Model Training and Evaluation: Employed cross-validation techniques to assess model performance and prevent overfitting.
 - Outcome:
   The project demonstrated that ensemble models significantly outperformed baseline models in detecting software faults, achieving higher accuracy and lower false positive rates. The findings provided insights into the effectiveness of different machine learning approaches for software reliability, contributing valuable knowledge to the field of software 
   engineering.


***Comprehensive Formula-1 Race Analysis: Team Performance & Financial Insights (1950-2023)*** [Link](https://github.com/kushagra-619/Comprehensive_Formula-1_Race_Analysis-Team_Performance_and_Financial_Insights_-1950-2023-)
 - The objective of this project was to analyze Formula 1 race data spanning from 1950 to 2023, focusing on team performance and financial insights. The analysis aimed to uncover significant financial disparities among teams and illustrate their evolution over time, providing actionable insights through comprehensive data visualization.
 - Data Collection: Gathered datasets from various sources, including race results, team financials, and driver statistics.
   Data Cleaning and Preparation: Merged multiple CSV files containing race and financial data. Removed duplicates, handled missing values, and standardized formats.
   Normalized numerical data to ensure consistency across datasets.
 - Exploratory Data Analysis (EDA):
   Created interactive dashboards in Tableau to visualize race results, team performance, and financial trends.
   Employed bar charts, line graphs, and scatter plots to illustrate key metrics.
   Conducted correlation analysis to identify relationships between performance and financial investments.
 - Advanced Analysis:
   Implemented trend analysis and forecasting tools to predict future performance.
   Developed heat maps to visualize performance variability across circuits and seasons.
   Used clustering techniques to group teams and drivers based on performance metrics.
 - Key Findings:
   Identified top-performing teams and drivers across different eras.
   Revealed significant financial gaps, highlighting the impact of budgets on performance.
   Illustrated the evolution of teams and tracked changes in performance due to rule changes and technological advancements.
 - Outcome:
   The project delivered detailed insights into the dynamics of Formula 1 racing, focusing on how financial resources affect team performance. It provided stakeholders with intuitive visualizations that facilitate informed decision-making and helped teams optimize performance and financial strategies.

***Project: E-commerce Database Management System*** [Link](https://github.com/kushagra-619/E-commerce_Database_Management_System)
 - This project involved the development of a comprehensive Database Management System (DBMS) tailored for an e-commerce platform. The system was designed to efficiently manage various aspects of online retail operations, including inventory management, order processing, customer information, product cataloging, and shipping logistics.
 - Business Requirements:
   Design a relational database structure to support e-commerce operations.
   Implement tables for products, inventory, orders, customers, sellers, and shipping.
   Establish relationships between different entities in the database.
   Develop stored procedures for efficient data insertion and management.
   Ensure data integrity through proper constraints and foreign key relationships.
 - Steps Taken:
   Database Design: Created tables for entities such as order_detail, inventory, product, category, condition, seller, orders, consumer, address, shipping_type, package, and shipment.
   Table Creation: Executed SQL commands to set up the necessary tables with appropriate columns and data types.
   Stored Procedures Development: Implemented stored procedures for adding data to various tables, including procedures like ADD_CONSUMER, ADD_INVENTORY, ADD_ORDERS, and others.
   Data Insertion: Used the created stored procedures to insert sample data into the tables for testing and validation.
   Relationship Establishment: Added foreign key constraints to ensure relationships between tables, maintaining data integrity and consistency.
   Query Implementation: Developed SQL queries to extract meaningful insights, such as inventory status, consumer statistics, and order details.
   Table Alterations: Modified existing tables to add necessary constraints and relationships.
   Testing and Validation: Executed stored procedures and queries to ensure proper functionality and accurate data retrieval.
 - Outcome:
   The project successfully established a robust database management system capable of supporting essential e-commerce operations, ensuring data integrity, and providing valuable business insights through efficient querying.

## Interests 

1. **I have a diverse range of interests that fuel my creativity and drive. As an avid sports enthusiast, I enjoy playing cricket, soccer, and tennis, which not only keep me active but also teach valuable lessons in teamwork and strategy. I am passionate about cinema and series, always eager to explore new narratives and cinematic styles.**
2. **My love for history and geopolitics keeps me informed about global events, enhancing my analytical skills and understanding of complex issues. Additionally, I am captivated by the thrill of Formula 1 racing, where precision and innovation collide.**
3. **Traveling allows me to experience different cultures and cuisines, while cooking various dishes at home allows me to express my creativity in the kitchen. Together, these interests shape my perspective, inspiring me to approach challenges with a well-rounded and informed mindset.**

## Side Projects

**Medical Insurance Price Prediction using Machine Learning**

- This project applies machine learning techniques to predict medical insurance prices based on individual characteristics. Using a dataset of 1338 entries with 7 features (age, sex, BMI, children, smoker status, region, and charges), the analysis aims to identify key factors influencing insurance costs and develop accurate predictive models.

- The project workflow includes exploratory data analysis, outlier detection and treatment, feature engineering, and model comparison. Several machine learning models were evaluated, including Linear Regression, SVR, Random Forest, Gradient Boosting, and XGBoost.

- Key findings reveal that males generally have higher insurance charges, smoking significantly impacts premiums, and charges vary across regions. The XGBoost model emerged as the most accurate predictor.

- This analysis has practical applications in creating personalized insurance packages, assessing risk profiles, developing competitive pricing strategies, and understanding customer behavior. The project demonstrates proficiency in data analysis, machine learning, and their practical application in the insurance industry.
   
- Future work could involve expanding the dataset for more nuanced insights, implementing explainable AI techniques, and developing dynamic pricing strategies based on real-time customer behavior.


**Data Visualization Using SPLUNK: Population, Mortality, and Nutrition Analysis**

- This project uses data visualization techniques to explore various aspects of global population dynamics, mortality rates, and nutrition issues. The analysis covers several key areas:

1. Population demographics in the USA, including male and female population trends across different age groups.

2. Mortality rates by gender and age, with a focus on causes of death, particularly cardiovascular diseases.

3. Mental health and substance use disorders, examining deaths in G7 nations and New York City.

4. Global nutrition issues, including malnutrition statistics for various countries and the Global Hunger Index.

- The project utilizes various chart types (bar charts, line graphs, maps) to present data effectively. It highlights important trends such as increasing life expectancy, gender-specific mortality rates, and the impact of mental health issues on different demographics. The nutrition analysis provides insights into calorie deficits and undernourishment across countries.

- This visualization project demonstrates skills in data analysis, chart creation, and the ability to present complex information in an accessible format.

**'Governance, Risk and Compliance(GRC)' Case Study**

- Focus of the Study: This case study explored the significance of Governance, Risk, and Compliance (GRC) in the corporate world, examining two specific companies that failed to implement proper GRC processes.
- Key Findings: Importance of GRC
  GRC is crucial for coordinating governance, risk management, and compliance activities.
  Proper GRC implementation leads to enhanced decision-making, reduced risk exposure, and improved regulatory compliance.
  A robust GRC framework fosters transparency, accountability, and stakeholder trust.

- Case Study 1: Lloyds and HBOS Merger.
Lack of due diligence led to unforeseen financial losses.
Unchecked bad practices at HBOS, including reckless lending, violated good governance.
A hidden loan scam caused significant damage to small businesses.
Consequences included massive financial losses, fines, lawsuits, and reputational damage.

- Case Study 2: Uber's Governance and Regulatory Challenges.
Founder's aggressive leadership style led to clashes with regulators globally.
Cultural issues, including allegations of sexual harassment, damaged the company's reputation.
Uber's business model faced regulatory challenges in multiple countries.
The company experienced poor stock performance and valuation criticism.

- Recommendations
Leadership Change: Appoint leaders committed to ethical practices and regulatory compliance.
Cultural Transformation: Foster a culture of transparency, accountability, and respect.
Regulatory Compliance Training: Provide comprehensive training on regulatory requirements.
Stakeholder Engagement: Establish open communication with regulators and other stakeholders.
Enhanced Governance Framework: Implement clear policies, procedures, and oversight mechanisms.
Global Regulatory Strategy: Develop a strategy that considers local regulations in all markets.
Ethical Business Practices: Reinforce zero-tolerance policies for unethical behavior.
Transparent Communication: Clearly communicate remediation efforts to all stakeholders.
Financial Stabilization: Develop sustainable financial strategies to regain investor confidence.
Continuous Monitoring and Adaptation: Implement robust systems to track compliance metrics and address emerging issues.

- Key Takeaway
The case study underscores the critical importance of a comprehensive GRC framework in safeguarding an organization's reputation, financial stability, and long-term success. It highlights the need for proactive risk management, ethical business practices, and continuous adaptation to regulatory changes.
